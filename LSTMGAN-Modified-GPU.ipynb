{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "music21: Certain music21 functions might need these optional packages: matplotlib, scipy;\n",
      "                   if you run into errors, install them by following the instructions at\n",
      "                   http://mit.edu/music21/doc/installing/installAdditional.html\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from music21 import *\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import memory_saving_gradients\n",
    "from random import shuffle\n",
    "import zipfile\n",
    "num_notes = 128\n",
    "length = 19200\n",
    "noisesize = 19200\n",
    "learning_rate = 0.001\n",
    "tf.__dict__[\"gradients\"] = memory_saving_gradients.gradients_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(X, num_filters, reuse = None):\n",
    "    with tf.variable_scope('dis',reuse=reuse):\n",
    "        layer1 = tf.layers.conv2d(inputs = X, filters = num_filters, kernel_size = [16,num_notes], padding = \"same\", activation = tf.nn.relu, strides = [1,2])\n",
    "        layer2 = tf.layers.conv2d(inputs = layer1, filters = num_filters*2, kernel_size = [8,num_notes], padding = \"same\", activation = tf.nn.relu, strides = [1,2])\n",
    "        layer3 = tf.layers.conv2d(inputs = layer2, filters = num_filters*4, kernel_size = [4,num_notes], padding = \"same\", activation = tf.nn.relu, strides = [1,2])\n",
    "        flat = tf.reshape(layer3, [-1, int(num_notes/8 * length * num_filters*4)])\n",
    "        dense = tf.layers.dense(inputs=flat, units=128, activation=tf.nn.relu)\n",
    "        dropout = tf.layers.dropout(inputs=dense, rate=0.4)\n",
    "        logits = tf.layers.dense(inputs=dropout, units=1)\n",
    "        output = tf.sigmoid(logits)\n",
    "        return output, logits\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, num_neurons, reuse = None):\n",
    "    with tf.variable_scope('gen',reuse=reuse):\n",
    "        cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(num_units=num_neurons, activation=tf.nn.relu6),\n",
    "        output_size=num_notes)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, z[:,:,:], dtype=tf.float32)\n",
    "        outpus = outputs/6\n",
    "        #lastoutput = outputs[:,-1,:]\n",
    "        #bias_tensor = tf.fill(tf.shape(lastoutput), .5-a)\n",
    "        #lastoutput = tf.add(lastoutput, bias_tensor)\n",
    "        #lastoutput = tf.round(lastoutput)\n",
    "        #reshapedlastout = tf.reshape(lastoutput,[-1, 1 , num_notes])\n",
    "        #reshapedlastout = reshapedlastout/6\n",
    "        #z = tf.concat([z,reshapedlastout],1)\n",
    "        #for i in range(length-1):\n",
    "        #    print(i)\n",
    "        #    outputs, states = tf.nn.dynamic_rnn(cell, z[:,:,-noisesize:], dtype=tf.float32)\n",
    "        #    lastoutput = outputs[:,-1,:]\n",
    "        #    reshapedlastout = tf.reshape(lastoutput,[-1,1 , num_notes])\n",
    "        #    reshapedlastout = reshapedlastout/6\n",
    "        #    z = tf.concat([z,reshapedlastout],1)\n",
    "        #z = z[:,-length:,:]\n",
    "        return tf.reshape(outputs, [-1,19200,128,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(array, filename):\n",
    "    ray = array\n",
    "    #print(ray.shape)\n",
    "    #ray = ray.reshape(19200,128)\n",
    "    ray = ray.reshape(128,19200)\n",
    "    stream1 = stream.Stream()\n",
    "    for x in range(ray.shape[1]):\n",
    "        for y in range(ray.shape[0]):\n",
    "            if (ray[y,x] == 1) and (ray[y,x-1] == 0):\n",
    "                w = 0\n",
    "                while ray[y, x+w] == 1:\n",
    "                    w += 1\n",
    "                    if x+w >= 19200:\n",
    "                        break\n",
    "                w += 2\n",
    "                note1 = note.Note(y) #check this\n",
    "                note1.quarterLength = w/24\n",
    "                stream1.append(note1)\n",
    "                note1.offset = x/24 #is this the right way?\n",
    "    stream1.write(\"midi\", \"./outputsreals/\" + str(filename) + \".mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(lis):\n",
    "    counter = 0\n",
    "    string = \"\"\n",
    "    for item in lis:\n",
    "        string = str(item)\n",
    "        zipfilePath = (\"./numpys/\" + string + \".pickle.zip\")\n",
    "        zip = zipfile.ZipFile(zipfilePath)\n",
    "        zip.extractall(\"./numpys\")\n",
    "        pickle_file = open(\"./numpys/\" + string + \".pickle\",\"rb\")\n",
    "        lis = pickle.load(pickle_file)\n",
    "        pickle_file.close()\n",
    "        ray = np.array(lis)\n",
    "        if counter == 0:\n",
    "            print(\"made final\")\n",
    "            final = np.reshape(ray, [1,length,num_notes,1])\n",
    "        else:\n",
    "            print(\"catted\")\n",
    "            final = np.concatenate((final, np.reshape(ray, [1,length,num_notes,1])), 0)\n",
    "            print(final.shape)\n",
    "        os.remove(\"./numpys/\" + string + \".pickle\")\n",
    "        counter = counter + 1\n",
    "    return (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made final\n",
      "catted\n",
      "(2, 19200, 128, 1)\n",
      "catted\n",
      "(3, 19200, 128, 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    real_music = tf.placeholder(tf.float32, shape = [None, length, num_notes,1])\n",
    "    z = tf.placeholder(tf.float32,shape=[None,noisesize, num_notes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0222 21:43:36.130717 140735995515776 deprecation.py:323] From <ipython-input-3-7f582c5828ba>:4: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0222 21:43:36.138009 140735995515776 deprecation.py:323] From <ipython-input-3-7f582c5828ba>:6: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0222 21:43:36.232287 140735995515776 deprecation.py:506] From /anaconda3/envs/mypython3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1253: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0222 21:43:36.243517 140735995515776 deprecation.py:506] From /anaconda3/envs/mypython3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:734: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0222 21:43:36.279677 140735995515776 deprecation.py:506] From /anaconda3/envs/mypython3/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell.py:104: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    g = generator(z,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0221 21:27:57.638404 140735995515776 deprecation.py:323] From <ipython-input-2-a10511fb4723>:3: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.layers.Conv2D instead.\n",
      "W0221 21:27:57.701711 140735995515776 deprecation.py:323] From <ipython-input-2-a10511fb4723>:7: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0221 21:27:57.725934 140735995515776 deprecation.py:323] From <ipython-input-2-a10511fb4723>:8: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    D_output_real , D_logits_real = discriminator(real_music, 2)\n",
    "    D_output_fake, D_logits_fake = discriminator(g,2,reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(logits_in,labels_in):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in,labels=labels_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    D_real_loss = loss_func(D_logits_real,tf.ones_like(D_logits_real)* (0.9))\n",
    "    D_fake_loss = loss_func(D_logits_fake,tf.zeros_like(D_logits_real))\n",
    "    D_loss = D_real_loss + D_fake_loss\n",
    "    G_loss = loss_func(D_logits_fake,tf.ones_like(D_logits_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dis/conv2d/kernel:0', 'dis/conv2d/bias:0', 'dis/conv2d_1/kernel:0', 'dis/conv2d_1/bias:0', 'dis/conv2d_2/kernel:0', 'dis/conv2d_2/bias:0', 'dis/dense/kernel:0', 'dis/dense/bias:0', 'dis/dense_1/kernel:0', 'dis/dense_1/bias:0']\n",
      "['gen/rnn/output_projection_wrapper/basic_lstm_cell/kernel:0', 'gen/rnn/output_projection_wrapper/basic_lstm_cell/bias:0', 'gen/rnn/output_projection_wrapper/kernel:0', 'gen/rnn/output_projection_wrapper/bias:0']\n"
     ]
    }
   ],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "\n",
    "d_vars = [var for var in tvars if 'dis' in var.name]\n",
    "g_vars = [var for var in tvars if 'gen' in var.name]\n",
    "\n",
    "print([v.name for v in d_vars])\n",
    "print([v.name for v in g_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    D_trainer = tf.train.AdamOptimizer(learning_rate).minimize(D_loss, var_list=d_vars)\n",
    "    G_trainer = tf.train.AdamOptimizer(learning_rate).minimize(G_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_songs = 20425\n",
    "epochs = 100\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(var_list=g_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar(\"D loss\", D_loss)\n",
    "tf.summary.scalar(\"G loss\", G_loss)\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a80b201ad24e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inited\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./realoutput1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"inited\")\n",
    "    writer = tf.summary.FileWriter(\"./realoutput1\",sess.graph)\n",
    "    for e in range(epochs):\n",
    "        num_batches = num_songs // batch_size\n",
    "        lis = list(range(num_songs))\n",
    "        shuffle(lis)\n",
    "        for i in range(num_batches):\n",
    "            #print(i)\n",
    "            batch = next_batch(lis[i*batch_size:i*batch_size + batch_size])\n",
    "            \n",
    "            \n",
    "            batch_z = np.random.uniform(0, 1, size=[batch_size, noisesize, num_notes]) #not sure if 0 or -1\n",
    "            \n",
    "            _ = sess.run(D_trainer, feed_dict={real_music: batch, z: batch_z})\n",
    "            _ = sess.run(G_trainer, feed_dict={z: batch_z})\n",
    "            summaries = sess.run(merged_summary_op, feed_dict={real_music: batch, z: batch_z})\n",
    "            writer.add_summary(summaries, e * num_batches + i)\n",
    "            if i%100 == 0:\n",
    "                print(\"100 done!\")\n",
    "                #losd = sess.run(D_loss,feed_dict={real_music: batch_data, z: batch_z})\n",
    "                #losg = sess.run(G_loss,feed_dict={real_music: batch_data, z: batch_z})\n",
    "                #print(\"Generator loss: \" + str(losg))\n",
    "                #print(\"Discriminatior loss: \" + str(losd))\n",
    "        print(\"Currently on Epoch {} of {} total...\".format(e+1, epochs))\n",
    "        sample_z = np.random.uniform(0, 1, size=[1, noisesize, num_notes]) \n",
    "        gen_sample = sess.run(generator(z, reuse = True),feed_dict={z: sample_z})\n",
    "        reconstruct(gen_sample, e)\n",
    "        saver.save(sess, './checkpoints')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
