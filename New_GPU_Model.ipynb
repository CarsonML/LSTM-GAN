{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from music21 import *\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from random import shuffle\n",
    "import zipfile\n",
    "num_notes = 128\n",
    "length = 19200\n",
    "noisesize = 19200\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(X, num_filters, reuse = None):\n",
    "    with tf.variable_scope('dis',reuse=reuse):\n",
    "        layer1 = tf.layers.conv2d(inputs = X, filters = num_filters, kernel_size = [48,16], padding = \"same\", activation = tf.nn.relu, strides = [24,1])\n",
    "        layer2 = tf.layers.conv2d(inputs = layer1, filters = num_filters*2, kernel_size = [16,8], padding = \"same\", activation = tf.nn.relu, strides = [2,2])\n",
    "        layer3 = tf.layers.conv2d(inputs = layer2, filters = num_filters*4, kernel_size = [8,4], padding = \"same\", activation = tf.nn.relu, strides = [1,2])\n",
    "        flat = tf.reshape(layer3, [-1, int(num_notes/192 * length * num_filters*4)])\n",
    "        dense = tf.layers.dense(inputs=flat, units=64, activation=tf.nn.relu)\n",
    "        dropout = tf.layers.dropout(inputs=dense, rate=0.4)\n",
    "        logits = tf.layers.dense(inputs=dropout, units=1)\n",
    "        output = tf.sigmoid(logits)\n",
    "        return output, logits\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, num_neurons, reuse = None):\n",
    "    with tf.variable_scope('gen',reuse=reuse):\n",
    "        cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(num_units=num_neurons, activation=tf.nn.relu6),\n",
    "        output_size=num_notes)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, z[:,:,:], dtype=tf.float32)\n",
    "        scalar = (1/6)\n",
    "        divided = tf.math.scalar_mul(scalar, outputs)\n",
    "        #lastoutput = outputs[:,-1,:]\n",
    "        #bias_tensor = tf.fill(tf.shape(lastoutput), .5-a)\n",
    "        #lastoutput = tf.add(lastoutput, bias_tensor)\n",
    "        #lastoutput = tf.round(lastoutput)\n",
    "        #reshapedlastout = tf.reshape(lastoutput,[-1, 1 , num_notes])\n",
    "        #reshapedlastout = reshapedlastout/6\n",
    "        #z = tf.concat([z,reshapedlastout],1)\n",
    "        #for i in range(length-1):\n",
    "        #    print(i)\n",
    "        #    outputs, states = tf.nn.dynamic_rnn(cell, z[:,:,-noisesize:], dtype=tf.float32)\n",
    "        #    lastoutput = outputs[:,-1,:]\n",
    "        #    reshapedlastout = tf.reshape(lastoutput,[-1,1 , num_notes])\n",
    "        #    reshapedlastout = reshapedlastout/6\n",
    "        #    z = tf.concat([z,reshapedlastout],1)\n",
    "        #z = z[:,-length:,:]\n",
    "        return tf.reshape(divided, [-1,19200,128,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(array, filename):\n",
    "    ray = array\n",
    "    #print(ray.shape)\n",
    "    #ray = ray.reshape(19200,128)\n",
    "    ray = ray.reshape(128,19200)\n",
    "    print(ray[:,5])\n",
    "    stream1 = stream.Stream()\n",
    "    for x in range(ray.shape[1]):\n",
    "        for y in range(ray.shape[0]):\n",
    "            if (ray[y,x] == 1) and (ray[y,x-1] == 0):\n",
    "                w = 0\n",
    "                while ray[y, x+w] == 1:\n",
    "                    w += 1\n",
    "                    if x+w >= 19200:\n",
    "                        break\n",
    "                w += 2\n",
    "                note1 = note.Note(y) #check this\n",
    "                note1.quarterLength = w/24\n",
    "                stream1.append(note1)\n",
    "                note1.offset = x/24 #is this the right way?\n",
    "    stream1.write(\"midi\", \"./sampleMusic/\" + filename + \".mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(lis):\n",
    "    counter = 0\n",
    "    string = \"\"\n",
    "    for item in lis:\n",
    "        string = str(item)\n",
    "        zipfilePath = (\"./numpys/\" + string + \".pickle.zip\")\n",
    "        zip = zipfile.ZipFile(zipfilePath)\n",
    "        zip.extractall(\"./numpys\")\n",
    "        pickle_file = open(\"./numpys/\" + string + \".pickle\",\"rb\")\n",
    "        lis = pickle.load(pickle_file)\n",
    "        pickle_file.close()\n",
    "        ray = np.array(lis)\n",
    "        if counter == 0:\n",
    "            #print(\"made final\")\n",
    "            final = np.reshape(ray, [1,length,num_notes,1])\n",
    "        else:\n",
    "            #print(\"catted\")\n",
    "            final = np.concatenate((final, np.reshape(ray, [1,length,num_notes,1])), 0)\n",
    "            #print(final.shape)\n",
    "        os.remove(\"./numpys/\" + string + \".pickle\")\n",
    "        counter = counter + 1\n",
    "    return (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.device('/gpu:0'):\n",
    "real_music = tf.placeholder(tf.float32, shape = [None, length, num_notes,1])\n",
    "z = tf.placeholder(tf.float32,shape=[None,noisesize, num_notes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.device('/gpu:0'):\n",
    "g = generator(z,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.device('/gpu:0'):\n",
    "D_output_real , D_logits_real = discriminator(real_music, 16)\n",
    "D_output_fake, D_logits_fake = discriminator(g,16,reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(logits_in,labels_in):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in,labels=labels_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.device('/gpu:0'):\n",
    "D_real_loss = loss_func(D_logits_real,tf.ones_like(D_logits_real)* (0.9))\n",
    "D_fake_loss = loss_func(D_logits_fake,tf.zeros_like(D_logits_real))\n",
    "D_loss = D_real_loss + D_fake_loss\n",
    "G_loss = loss_func(D_logits_fake,tf.ones_like(D_logits_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "\n",
    "d_vars = [var for var in tvars if 'dis' in var.name]\n",
    "g_vars = [var for var in tvars if 'gen' in var.name]\n",
    "\n",
    "print([v.name for v in d_vars])\n",
    "print([v.name for v in g_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.device('/gpu:0'):\n",
    "D_trainer = tf.train.AdamOptimizer(learning_rate).minimize(D_loss, var_list=d_vars)\n",
    "G_trainer = tf.train.AdamOptimizer(learning_rate).minimize(G_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_songs = 20425\n",
    "epochs = 100\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar(\"D loss\", D_loss)\n",
    "tf.summary.scalar(\"G loss\", G_loss)\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"inited\")\n",
    "    writer = tf.summary.FileWriter(\"./editedoutput\",sess.graph)\n",
    "    for e in range(epochs):\n",
    "        num_batches = num_songs // batch_size\n",
    "        lis = list(range(num_songs))\n",
    "        shuffle(lis)\n",
    "        for i in range(num_batches):\n",
    "            #print(i)\n",
    "            batch = next_batch(lis[i*batch_size:i*batch_size + batch_size])\n",
    "            \n",
    "            \n",
    "            batch_z = np.random.uniform(0, 1, size=[batch_size, noisesize, num_notes]) #not sure if 0 or -1\n",
    "            \n",
    "            _ = sess.run(D_trainer, feed_dict={real_music: batch, z: batch_z})\n",
    "            _ = sess.run(G_trainer, feed_dict={z: batch_z})\n",
    "            summaries = sess.run(merged_summary_op, feed_dict={real_music: batch, z: batch_z})\n",
    "            writer.add_summary(summaries, e * num_batches + i)\n",
    "            if i%1 == 0:\n",
    "                print(\"32 done!\")\n",
    "                #losd = sess.run(D_loss,feed_dict={real_music: batch_data, z: batch_z})\n",
    "                #losg = sess.run(G_loss,feed_dict={real_music: batch_data, z: batch_z})\n",
    "                #print(\"Generator loss: \" + str(losg))\n",
    "                #print(\"Discriminatior loss: \" + str(losd))\n",
    "            if i%10 == 0:\n",
    "                sample_z = np.random.uniform(0, 1, size=[1, noisesize, num_notes]) \n",
    "                gen_sample = sess.run(generator(z,128,reuse = True),feed_dict={z: sample_z})\n",
    "                reconstruct(gen_sample, str(e) + \",\" +str(i))\n",
    "                saver.save(sess, \"model.ckpt\")\n",
    "        print(\"Currently on Epoch {} of {} total...\".format(e+1, epochs))\n",
    "        sample_z = np.random.uniform(0, 1, size=[1, noisesize, num_notes]) \n",
    "        gen_sample = sess.run(generator(z,128, reuse = True),feed_dict={z: sample_z})\n",
    "        reconstruct(gen_sample, e)\n",
    "        saver.save(sess, './checkpoints/model.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
